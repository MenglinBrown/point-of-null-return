策略
Strategy
C - 准则 (Criteria)确定模型选择的准则, 即学习的策略
1. 损失函数loss function/代价函数cost function e.g. 0-1 loss, least mean squre LMS, abs loss, log loss
a. 损失函数定义：对给定输入X的预测值 $f(x)$ 和真实值 $Y$ 之间的 $Y$ 之间的非负实值函数, $L(Y, f(X)$ )
i. least mean square: $J(\theta)=\frac{1}{2} \sum_{i=1}^n\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$
ii. $\log$ loss:
2. 现在有了损失函数，损失函数数值越小，模型越好，损失函数的期望就是期望损失。学习的目标就是选择期望风险最小的模型。
a. 风险函数risk function和期望损失expected loss
i. 公式: $\quad R_{e x p}(f)=E_p[L(Y, f(X))]=\int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{~d} y$
ii. 如果知道联合分布 $P(X, Y)$ 就可以直接计算Inference出来, 不需要学习Learning, 所以学习学的就是这个联合分布 (cs221 reflex models: learning)
3. 但是现在不知道联合分布 $\mathrm{P}(\mathrm{x}, \mathrm{y})$, 所以需要用emperical loss, 来近似
a. 经验风险empirical risk和经验损失emprirical loss
i. 公式: $\quad R_{e m p}(f)=\frac{1}{N} \sum_{i=1}^N L\left(y_i, f\left(x_i\right)\right)$
ii. 期望风险时模型关于联合分布的期望损失, 经验风险时模型关于训练样本集的平均损失。
iii. 根据大数定律, 当样本容量 $\mathrm{N}$ 趋于无穷大时, 经验风险趋于期望风险。
b. 经验风险最小化(empirical risk minimization, ERM)
i. 公式: $\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N L\left(y_i, f\left(x_i\right)\right)$
(1.11) 其中F是假设空间
ii. 经验风险最小的就是最优的模型, 极大似然估计(maximum likelihood estimation, MLE)就是经验风险最小化的一个例子, 当模型是条件概率分布conditional distribution，损失函数是对数损失函数log loss的时候，经验风险最小化ERM等价于极大似然估计
4. 但是样本容量小的时候, 经验风险最小化学习容易出现过拟合overfitting, 所以用结构风险最小化SRM防止过拟合
a. 结构风险structural risk
i. 公式: $\quad R_{s r m}(f)=\frac{1}{N} \sum_{i=1}^N L\left(y_i, f\left(x_i\right)\right)+\lambda J(f)$
(1.12) 其中 $J(f)$ 为模型复杂度, $l a m b d a>=0$ 是系数, 用以权衡经验风险和模型复杂度
ii. 结构风险最小化SRM等价于正则化regularization:结构风险在经验风险上加上表示模型复杂度的正则化项regularizer/penalty term
b. 结构风险最小化(structural risk minimization, SRM)
i. 公式: $\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N L\left(y_i, f\left(x_i\right)\right)+\lambda J(f)$
ii. 贝叶斯估计中的最大后验概率估计(maximum posterior probability estimation, MAP) 就是SRM的一个例子。当模型是条件概率分布 conditional distribution、损失函数是对数损失函数log loss、模型复杂度由模型的先验概率表示的时候，结构风险最小化就等价于最大后验概率估计
5. 至此, 监督学习问题就成了经验风险或结构风险函数的最优化问题(1.11, 1.13), 这时经验或结构风险函数就是最优化的目标函数